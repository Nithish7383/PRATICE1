‚úÖ Project Setup & Installation Guide

Release Risk Intelligence ‚Äì Backend + LLM (Ollama)

This document lists ALL requirements and exact steps to run the project on a fresh machine.

1Ô∏è‚É£ SYSTEM REQUIREMENTS
Operating System

Windows 10 / 11 (64-bit)

macOS / Linux (Ollama supported)

Hardware (Minimum)

RAM: 8 GB (16 GB recommended)

Disk: 10 GB free space

CPU: Any modern 64-bit processor

‚ö†Ô∏è GPU is NOT mandatory (CPU works fine for Mistral).

2Ô∏è‚É£ SOFTWARE REQUIREMENTS
üîπ 1. Python

Python 3.10 ‚Äì 3.13

Verify:

python --version


If not installed ‚Üí install from:
üëâ https://www.python.org/downloads/

‚úîÔ∏è During install, check ‚ÄúAdd Python to PATH‚Äù

üîπ 2. Ollama (LLM Runtime)

Ollama runs the LLM locally.

Download & install:
üëâ https://ollama.com/download

Verify installation:

ollama --version


Start Ollama service (auto-starts on Windows after install):

ollama list

üîπ 3. Download Required LLM Model

Run once:

ollama pull mistral


Verify:

ollama list


Expected output:

mistral:latest

3Ô∏è‚É£ PROJECT SETUP
üîπ 1. Clone / Copy Project
backend_truth_engine_FIXED/
‚îÇ
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ ai/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama_llm.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ release_risk_ai.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompt_template.txt
‚îÇ   ‚îú‚îÄ‚îÄ service/
‚îÇ   ‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_results.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commit_activity.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ defects.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pipeline_runs.csv
‚îÇ
‚îú‚îÄ‚îÄ venv/
‚îî‚îÄ‚îÄ requirements.txt

4Ô∏è‚É£ PYTHON VIRTUAL ENVIRONMENT (RECOMMENDED)
üîπ Create venv
python -m venv venv

üîπ Activate venv
Windows (PowerShell)
venv\Scripts\activate


‚ö†Ô∏è If blocked by execution policy:

Set-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy RemoteSigned


Then activate again.

macOS / Linux
source venv/bin/activate

5Ô∏è‚É£ PYTHON DEPENDENCIES
üîπ requirements.txt (MANDATORY)

Create this file:

pandas
pydantic
requests
ollama

üîπ Install dependencies
pip install -r requirements.txt


Verify Ollama python client:

python -c "import ollama; print('ollama OK')"

6Ô∏è‚É£ OLLAMA PYTHON INTEGRATION (IMPORTANT)
app/ai/ollama_llm.py (FINAL VERSION)
import ollama

class OllamaLLM:
    def __init__(self, model="mistral"):
        self.model = model

    def generate(self, prompt: str) -> str:
        response = ollama.chat(
            model=self.model,
            messages=[
                {"role": "user", "content": prompt}
            ],
            options={
                "temperature": 0.3,
                "num_predict": 400
            }
        )
        return response["message"]["content"]


‚úîÔ∏è ollama must be running in background

7Ô∏è‚É£ RUN THE PROJECT

From project root:

python app/main.py


Expected output:

===== AI EXPLANATION =====
Explanation:
...
Key Risk Drivers:
...
Recommended Actions:
...

8Ô∏è‚É£ COMMON ERRORS & FIXES
‚ùå ModuleNotFoundError: ollama

‚úî Fix:

pip install ollama

‚ùå ollama.chat: connection refused

‚úî Fix:

ollama serve


(or restart Ollama app)

‚ùå release_id mismatch

‚úî Fix:
Ensure ALL CSV files contain the SAME release_id

BILLING_SERVICE_REL_2025_Q4

9Ô∏è‚É£ WHY THIS SETUP IS ENTERPRISE-READY

‚úî Runs fully offline
‚úî No API keys
‚úî No production data exposure
‚úî Deterministic outputs
‚úî Auditable prompt + metrics
‚úî Easy to deploy on any laptop

10Ô∏è‚É£ ONE-LINE SUMMARY (FOR DOCUMENTATION)

This project uses Python, rule-based SDLC metrics, and a locally hosted Mistral LLM via Ollama to generate explainable release risk insights without exposing production data or relying on external APIs.